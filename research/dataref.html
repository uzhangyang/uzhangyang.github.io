<!DOCTYPE html>

<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<title>dataref</title>
		<style type="text/css">
		pre code
		{
		color:rgb(127, 0, 85);
		line-height:1.4;
		font-family:Consolas;
		}

		pre comment
		{
		color:rgb(63, 127, 95);
		line-height:1.4;
		font-family:Consolas;
		}

		pre	{
		margin: 0;
		padding-left: 5px;
		padding-top: 5px;
		padding-bottom: 5px;
		border: 0;
		border: 1px dotted #785;
		background: #f5f5f5;
		line-height:1.4;
		font-family:Consolas;
		}

		.content{
			margin-left:250;
			margin-right:250;
		}
		</style>
	</head>
<body>
<div class="content">
<h1>An Empirical Study of LLM-based Refactoring
Consistency</h1>
<hr/>
<p style="text-align:justify">Behavioral consistency plays an important role in improving software evolution
efficiency and guaranteeing software reliability in modern software refactoring.
Although large language models (LLMs) demonstrate great potential in multiple software engineering tasks, including code generation, code completion, and
code repair, few works have been conducted on LLM-based code refactoring.
Furthermore, existing works are confused about how LLMs impact refactoring
consistency. Therefore, there is an urgent need to conduct a systematic evaluation of behavioral consistency before and after refactoring. To this end, this
paper conducts the first empirical study on LLM-based refactoring consistency.
Firstly, a high-quality dataset DataRef is constructed with 468 Java and 544
Python code segments. Secondly, these code segments are refactored by existing
LLMs, such as ChatGPT3.5/4.0, CodeLlama, and CodeGeeX. Results demonstrate that a total of 1,108 refactored code segments have inconsistent symptoms.
Then, we create a taxonomy to classify these symptoms based on inconsistency
patterns. Thirdly, to study the refactoring ability of the latest LLM, we construct
a new dataset DataRef+ including 272 Java and 297 Python code segments and
refactor them with DeepSeek-V3/R1 and Qwen2.5-32B/72B. The experimental
results illustrate that 6.06% of code segments suffer from inconsistencies. Finally,
a mitigating strategy is proposed by combining retrieval-augmented generation
and structured few-shot prompting, which reduces inconsistency by 12.46%. Our
work provides empirical evidence for enhancing consistency before and after
refactoring and paves the way for future LLM-related refactoring.
</p>
<br>
<p style="text-align:justify">This paper conducts empirical research on the consistency before and after code
refactoring. Firstly, high-quality programs are selected from GitHub and LeetCode
to construct a high-quality dataset, which contains 468 Java code segments and 544
Python code segments. Secondly, LLMs such as ChatGPT3.5/4.0, CodeLlama, and
CodeGeeX are used to refactor these code segments. For the refactored code segments,
we employ the method of differential testing to detect inconsistencies between the original and refactored versions. Subsequently, we adopt the open
card sorting method to conduct detailed testing and comparison of
the code with inconsistent behaviors, and classify the causes of behavioral inconsistencies in the code refactored by LLMs. Finally, to further investigate the inconsistency
issues of the latest LLMs, we construct an inconsistency set based on the above work
and use this set to evaluate the refactoring capabilities of LLMs such as DeepSeekV3, DeepSeek-R1, Qwen2.5-32B, and Qwen2.5-72B. Figure 1 illustrates the overall
framework.</p>
<div style="text-align: center; margin: auto; width: 50%;"> <!-- 居中显示的容器 -->
  <img src="https://raw.githubusercontent.com/waaylj/DataRef/main/Method1.png"  border="0" style="max-width: 100%; height: auto;">
  <div style="text-align: center; margin-top: 10px;">Fig. 1: Framework Overview</div>
</div>

<div style="text-align: center; margin: auto; width: 50%;"> <!-- 居中显示的容器 -->
</div>

<h2>Source code and dataset</h2>
<hr/>
<p style="text-align:justify">The source code is available at the following link
 <a href="https://github.com/waaylj/DataRef/blob/main/Source%20Code.zip">SourceCode</a></p>
<p style="text-align:justify">The DataRef dataset is available at the following link <a href="https://github.com/waaylj/DataRef/blob/main/DataRef.zip">DataRef</a></p>
<p style="text-align:justify">The DataRef+ dataset is available at the following link <a href="https://github.com/waaylj/DataRef/blob/main/DataRef%2B.zip">DataRef+</a></p>
<p style="text-align:justify">
Please cite our work if you are interested in the source code and dataset.<br>
<br>

</div>
<img src="https://s01.flagcounter.com/count2/HbVu/bg_FFFFFF/txt_000000/border_CCCCCC/columns_4/maxflags_4/viewers_0/labels_1/pageviews_1/flags_0/percent_0/"  border="0">

</body>
</html>
